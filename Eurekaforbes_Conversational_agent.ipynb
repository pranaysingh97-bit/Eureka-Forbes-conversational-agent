{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8DWfZihZ00g",
        "outputId": "ab923f2f-a995-462a-f9c4-f62d3c06dbe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install google-genai faiss-cpu sentence-transformers pandas --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google import genai\n",
        "from IPython.display import clear_output # Needed for clear_output()\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Prompt the user for the API key\n",
        "os.environ[\"GEMINI_API_KEY\"] = input(\"Enter your Gemini API key: \")\n",
        "clear_output() # Clears the input prompt and key from the output\n",
        "\n",
        "# Initialize the Gemini Client\n",
        "try:\n",
        "    client = genai.Client()\n",
        "    print(\"✅ Setup Complete: Gemini Client initialized and API Key loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR: Could not initialize client. Please check the API key you entered. Error: {e}\")\n",
        "\n",
        "# Define the models we will use\n",
        "EMBEDDING_MODEL = 'text-embedding-004' # A strong embedding model\n",
        "\"\"\"\n",
        "Text-embedding-004, generates a vector that is 768 dimensions long.\n",
        "Therefore, each row (chunk) contains 768 numerical values, resulting in 768 columns.\n",
        "\"\"\"\n",
        "GENERATION_MODEL = 'gemini-2.5-flash' # A fast and capable generation model\n",
        "\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"- Embedding Model: {EMBEDDING_MODEL}\")\n",
        "print(f\"- Generation Model: {GENERATION_MODEL}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcxTafQIbpbK",
        "outputId": "161f38d4-241b-44c1-9145-5b6b3751cc7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Setup Complete: Gemini Client initialized and API Key loaded.\n",
            "\n",
            "Configuration:\n",
            "- Embedding Model: text-embedding-004\n",
            "- Generation Model: gemini-2.5-flash\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FAQ - Text splitting/chunking**"
      ],
      "metadata": {
        "id": "MEsuVWoqgQf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_corpus_text = \"\"\"\n",
        "Q1. What does Eureka Forbes offer?\n",
        "Eureka Forbes provides home health and hygiene solutions including water purifiers (AquaGuard & AquaSure), vacuum cleaners, air purifiers, security systems, and home service plans.\n",
        "\n",
        "Q2. What is the difference between AquaGuard and AquaSure?\n",
        "AquaGuard – premium range with advanced purification technologies and IoT-enabled features.\n",
        "AquaSure – value-for-money models offering essential purification.\n",
        "\n",
        "Q3. How do I choose the right water purifier for my home?\n",
        "It depends on your water source:\n",
        "RO – Borewell/Hard water (TDS > 500 ppm)\n",
        "UV – Municipal/Soft water (TDS < 200 ppm)\n",
        "RO+UV/RO+UV+UF – Mixed water supply\n",
        "You can also request a free water test.\n",
        "\n",
        "Q4. How often should filters be replaced?\n",
        "Typically every 6–12 months, depending on water quality and usage. The purifier alerts you when filters need replacement.\n",
        "\n",
        "Q5. What is the TDS range suitable for drinking water?\n",
        "As per BIS standards: 50–150 ppm is ideal for drinking.\n",
        "\n",
        "Q6. My AquaGuard is showing an error alert. What should I do?\n",
        "Most alerts indicate filter change or sensor issues. You can:\n",
        "Restart the unit\n",
        "Check the display panel\n",
        "Book a service request via Eureka Forbes app, customer care, or website\n",
        "\n",
        "Q7. How do I book a service request?\n",
        "You can book through:\n",
        "Eureka Forbes website\n",
        "Mobile app\n",
        "Customer care number: 1860 266 1177\n",
        "WhatsApp service (where available)\n",
        "\n",
        "Q8. What is the average installation time?\n",
        "Installation is usually completed within 24–48 hours after delivery.\n",
        "\n",
        "Q9. Are installation and demo free?\n",
        "For most products, installation is free\n",
        "Some models require paid consumables (pre-filter housing, extra pipes, booster pump, etc.)\n",
        "\n",
        "Q10. What is the warranty on Eureka Forbes products?\n",
        "Water purifiers: 1 year warranty (+ extended warranty available)\n",
        "Vacuum cleaners: 1–2 years depending on model\n",
        "\n",
        "Q11. What does the warranty cover?\n",
        "It covers manufacturing defects, electrical failures, and service technician visits. It does not cover consumables such as filters.\n",
        "\n",
        "Q12. What is AMC (Annual Maintenance Contract)?\n",
        "AMC includes:\n",
        "Periodic services\n",
        "Free filter replacement\n",
        "Free technician visits\n",
        "Priority support\n",
        "Ideal for maintaining RO purifiers.\n",
        "\n",
        "Q13. What types of vacuum cleaners does Eureka Forbes offer?\n",
        "Wet & Dry Vacuum Cleaners\n",
        "Handheld Vacuums\n",
        "Robotic Vacuum Cleaners\n",
        "Upright and Stick Vacuums\n",
        "\n",
        "Q14. How often should vacuum filters be cleaned?\n",
        "For best performance, clean after every 2–3 uses and replace every 6–12 months depending on usage.\n",
        "\n",
        "Q15. Are spare parts easily available?\n",
        "Yes, all genuine spare parts and accessories are available through authorized service centres and the official website.\n",
        "\n",
        "Q16. What does the air purifier remove?\n",
        "Removes PM2.5, dust, smoke, allergens, pet dander, VOCs, and odours using multi-layer filtration including HEPA filters.\n",
        "\n",
        "Q17. How often should the HEPA filter be replaced?\n",
        "Usually every 9–12 months, depending on usage and air quality.\n",
        "\n",
        "Q18. What payment methods are accepted?\n",
        "Credit/Debit Cards, UPI, Net Banking, EMI (No-cost EMI on select models), and COD in select cities.\n",
        "\n",
        "Q19. Can I return or replace a product?\n",
        "Returns are allowed under the return policy terms if the product is defective or not as expected. Replacement is processed after technician verification.\n",
        "\n",
        "Q20. Why is my RO water flow low?\n",
        "Possible reasons:\n",
        "Choked filters\n",
        "Low inlet water pressure\n",
        "Blocked membrane\n",
        "Pump malfunction\n",
        "A technician visit is recommended.\n",
        "\n",
        "Q21. Water tastes odd after filter change. Why?\n",
        "This is normal for the first 10–15 litres. Flush the purifier before drinking.\n",
        "\n",
        "Q22. My purifier is leaking. What should I do?\n",
        "Switch off the unit and inlet valve, then raise a service request. Usually caused by loose pipes or worn-out connectors.\n",
        "\n",
        "Q23. How can I contact Eureka Forbes customer care?\n",
        "Call: 1860 266 1177\n",
        "Website: www.eurekaforbes.com\n",
        "App: Eureka Forbes Service App\n",
        "Email support varies by zone (can be added if needed).\n",
        "\"\"\"\n",
        "\n",
        "# Split the text by the new Q#. pattern to create clean chunks\n",
        "\n",
        "import re #Regression expression module\n",
        "\n",
        "# Split by the pattern Q[number]. followed by a newline, keeping the pattern for now\n",
        "chunks = re.split(r'(\\nQ\\d+\\. )', raw_corpus_text)\n",
        "\n",
        "# Reconstruct the Q&A pairs and clean up empty strings\n",
        "clean_chunks = []\n",
        "for i in range(1, len(chunks), 2):\n",
        "    # Combine the Q#. text (chunks[i]) with the answer text (chunks[i+1])\n",
        "    clean_chunks.append((chunks[i].strip() + chunks[i+1].strip()).replace('\\n', ' '))\n",
        "\n",
        "# Final list of chunks for embedding\n",
        "final_chunks = [chunk for chunk in clean_chunks if chunk]\n",
        "\n",
        "print(f\"Total initial Q&A pairs (chunks) created: {len(final_chunks)}\")\n",
        "print(f\"Example of a cleaned chunk:\\n{final_chunks[22]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_UB_uTfbpFt",
        "outputId": "8ad41f97-ec2b-41e9-84a2-d5ebb930686c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total initial Q&A pairs (chunks) created: 23\n",
            "Example of a cleaned chunk:\n",
            "Q23.How can I contact Eureka Forbes customer care? Call: 1860 266 1177 Website: www.eurekaforbes.com App: Eureka Forbes Service App Email support varies by zone (can be added if needed).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EMBEDDING (generating vecotrs)"
      ],
      "metadata": {
        "id": "3DNPUllLhQkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary type hint\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# Function to generate embeddings using the Gemini API\n",
        "def embed_text(texts: List[str]) -> List[List[float]]:\n",
        "    \"\"\"Generates embeddings for a list of texts using the Gemini API.\"\"\"\n",
        "    global client # Ensure we are using the globally defined client\n",
        "    try:\n",
        "        response = client.models.embed_content( #This is the actual call to the Google Gemini API.\n",
        "            model=EMBEDDING_MODEL, #specifies which embedding model of gemini to use\n",
        "            contents=texts\n",
        "        )\n",
        "\n",
        "        # Use a list comprehension to pull the numerical vector out of each object\n",
        "        numerical_embeddings = [emb.values for emb in response.embeddings] #It iterates through the list of response objects and uses .values to extract the raw list of 768 floating-point numbers (the actual vector) from each one.\n",
        "\n",
        "        return numerical_embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during embedding: {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"Generating embeddings for all 23 chunks...\")\n",
        "# 1. Generate embeddings for all chunks\n",
        "chunk_embeddings = embed_text(final_chunks)\n",
        "chunk_embeddings_np = np.array(chunk_embeddings, dtype='float32') #It converts it into a NumPy array (chunk_embeddings_np).The dtype='float32' specifies the data type. FAISS is highly optimized for float32 (single-precision floating point numbers), making this conversion essential for performance and compatibility.\n",
        "\n",
        "# Verification\n",
        "print(\"\\n--- Embedding Results ---\")\n",
        "print(f\"Number of embeddings generated: {len(chunk_embeddings)}\")\n",
        "if chunk_embeddings:\n",
        "    print(f\"Dimension of each embedding vector: {len(chunk_embeddings[0])}\")\n",
        "    print(\"✅ Embedding successful!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUGPjD8Pbo5m",
        "outputId": "5c22a3ff-9a99-4fd9-f9ec-7d52a390672c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings for all 23 chunks...\n",
            "\n",
            "--- Embedding Results ---\n",
            "Number of embeddings generated: 23\n",
            "Dimension of each embedding vector: 768\n",
            "✅ Embedding successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Indexing (FAISS Vector Store) Facebook AI Similarity Search"
      ],
      "metadata": {
        "id": "qZfvKnXVhpnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Block C (Indexing)\n",
        "# 1. Determine the vector dimension\n",
        "embedding_dimension = chunk_embeddings_np.shape[1] #This is the NumPy array containing all your vectors, generated in the previous step. It's a 2D array (a matrix) where: 23 rows, 768 col\n",
        "\n",
        "# 2. Create the FAISS Index\n",
        "index = faiss.IndexFlatL2(embedding_dimension)\n",
        "# faiss.IndexFlatL2(...): This creates the actual searchable index.\n",
        "# IndexFlat: This type of index means the search will be a \"brute-force\" search (checking every vector). For small datasets (like your 23 chunks), this is the simplest and most accurate method.\n",
        "# L2: Specifies the distance metric used for comparison. L2 stands for Euclidean Distance (the straight-line distance between two points). In semantic search, a smaller L2 distance means the two vectors (query and chunk) are more semantically similar\n",
        "\n",
        "# 3. Add the embeddings to the index\n",
        "index.add(chunk_embeddings_np)\n",
        "\n",
        "# Verification\n",
        "print(\"\\n--- FAISS Indexing Results ---\")\n",
        "print(f\"FAISS index created with dimension: {embedding_dimension}\")\n",
        "print(f\"Total vectors in the index: {index.ntotal}\")\n",
        "\n",
        "# Save the chunks and index for easy access in the next step\n",
        "corpus_chunks = final_chunks\n",
        "print(\"\\n✅ RAG Corpus Preparation Complete: Chunks Embedded and Indexed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvElim0mbnrs",
        "outputId": "0eb9aa01-d7ee-47e1-e6b3-93e3f556d71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- FAISS Indexing Results ---\n",
            "FAISS index created with dimension: 768\n",
            "Total vectors in the index: 23\n",
            "\n",
            "✅ RAG Corpus Preparation Complete: Chunks Embedded and Indexed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Building RAG**\n",
        "##Setting K=6 which helps in taking top 6 similar chunks.\n",
        "If we reduce it 3 then the answer might not be meaningful. In this case if we ask two questions at a time, model only answers first questions as similarity will be limited top 3 chunks.\n",
        "\n",
        "**To reduce Hallucination** - Gave system prompt to LLM and asked it to stay strictly within in context and verify before answering.\n",
        "\n",
        "Initially output produced by AI Agent mentioned \"As per the data provided\". Hence, asked AI agent to not use it in the prompt itself, which helped in solving this problem."
      ],
      "metadata": {
        "id": "NNJyktS-3utt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Building the RAG Function ---\n",
        "from typing import Tuple, List\n",
        "\n",
        "# Define the number of chunks to retrieve for context\n",
        "TOP_K = 6\n",
        "\n",
        "def get_rag_context(query: str, k: int = TOP_K) -> str:\n",
        "    \"\"\"\n",
        "    1. Embeds the user query.\n",
        "    2. Searches the FAISS index for the top k chunks.\n",
        "    3. Returns the concatenated context string.\n",
        "    \"\"\"\n",
        "    global index, corpus_chunks, client # Use global variables defined in Step 3\n",
        "\n",
        "    # 1. Embed the query (using the same embed_text function, but only for the query)\n",
        "    # We need to reuse the embed_text function from Code Block B\n",
        "    query_embedding = embed_text([query]) # Embed the query list\n",
        "\n",
        "    # Convert to NumPy array for FAISS\n",
        "    query_embedding_np = np.array(query_embedding, dtype='float32')\n",
        "\n",
        "    # 2. Search the FAISS index\n",
        "    # D: Distances, I: Indices\n",
        "    D, I = index.search(query_embedding_np, k) #This is the core search command. k (set to 6) tells FAISS to return the 6 most similar vectors.\n",
        "    # D = Euclidean distance (similarity score) for the top 6 matches.\n",
        "    # I = A NumPy array containing the index position (0 to 22) of the top 6 matching chunks in the corpus_chunks list.\n",
        "\n",
        "    # 3. Retrieve the corresponding text chunks\n",
        "    retrieved_chunks = [corpus_chunks[i] for i in I[0]]\n",
        "\n",
        "    # Joins the 6 retrieved chunks into one large string, separated by \\n---\\n for clean formatting, which is then returned to the generation function.\n",
        "    context = \"\\n---\\n\".join(retrieved_chunks)\n",
        "    # print(context) #To understand the what 6 chunks it is similar to\n",
        "    return context\n",
        "\n",
        "def generate_rag_response(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Performs the full RAG process (Retrieval + Generation).\n",
        "    This function combines the retrieved context with the user's query and sends it to the Gemini model for a factual answer.\n",
        "    \"\"\"\n",
        "    # Retrieve the context\n",
        "    retrieved_context = get_rag_context(query)\n",
        "\n",
        "    # 3. Construct the prompt for the LLM\n",
        "    \"\"\"\n",
        "    This is the crucial Instruction for the LLM.\n",
        "    It dictates the bot's persona (\"expert conversational AI bot for Eureka Forbes\") and sets strict constraints:\n",
        "    MUST be based STRICTLY on the context, Do not use external knowledge, and maintain a \"professional and helpful\" tone\n",
        "    \"\"\"\n",
        "    system_prompt = (\n",
        "        \"You are an expert conversational AI bot for Eureka Forbes, designed to answer customer queries. \"\n",
        "        \"Your response MUST be based STRICTLY on the context provided below.No need to mention as per the data provided. \"\n",
        "        \"Do not use external knowledge. If the answer is not in the context, state that clearly but politely that you dont know about it and please call a toll free number 1860 266 1177 which is also available on WhatsApp. \"\n",
        "        \"Keep the tone professional and helpful.\"\n",
        "    )\n",
        "\n",
        "    full_prompt = (\n",
        "        f\"{system_prompt}\\n\\n\"  #Define guardrails\n",
        "        f\"--- CONTEXT START ---\\n\"\n",
        "        f\"{retrieved_context}\\n\"\n",
        "        f\"--- CONTEXT END ---\\n\\n\"\n",
        "        f\"CUSTOMER QUERY: {query}\"\n",
        "    )\n",
        "\n",
        "    # 4. Generate the response using Gemini-2.5-Flash\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=GENERATION_MODEL,\n",
        "            contents=full_prompt,\n",
        "        )\n",
        "        # Ensure the response object is handled correctly (we expect a string for the text)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Sorry, an error occurred during generation: {e}\"\n",
        "\n",
        "print(\"✅ RAG Retrieval and Generation functions defined.\")\n",
        "\n",
        "# --- Test the RAG Function ---\n",
        "test_query = \"How often should I change my filters and why is my water pressure low?\"\n",
        "test_response = generate_rag_response(test_query)\n",
        "\n",
        "print(\"\\n--- Test Query Results ---\")\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"Response:\\n{test_response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YIlH8VnboS0",
        "outputId": "9c7ee730-bfc2-43f7-e8dc-bd79a99f341d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ RAG Retrieval and Generation functions defined.\n",
            "\n",
            "--- Test Query Results ---\n",
            "Query: How often should I change my filters and why is my water pressure low?\n",
            "Response:\n",
            "Filters should typically be replaced every 6–12 months, depending on water quality and usage. Your purifier will alert you when the filters need replacement.\n",
            "\n",
            "If your RO water flow is low, possible reasons include choked filters, low inlet water pressure, a blocked membrane, or a pump malfunction. A technician visit is recommended for this issue.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5: Conversational Loop ---\n",
        "\n",
        "# IMPORTANT: Ensure that the following global variables are defined from previous steps:\n",
        "# - client (Gemini API client)\n",
        "# - GENERATION_MODEL\n",
        "# - EMBEDDING_MODEL\n",
        "# - index (FAISS index)\n",
        "# - corpus_chunks (List of text chunks)\n",
        "# - embed_text (Function from Code Block B)\n",
        "# - generate_rag_response (Function from Code Block D)\n",
        "\n",
        "print(\"--- Eureka Forbes RAG Chatbot Initialized ---\")\n",
        "print(\"You can now ask questions about the Eureka Forbes documentation.\")\n",
        "print(\"Type 'quit' or 'exit' to end the session.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Start the interactive loop\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"You: \").strip()\n",
        "\n",
        "        if user_input.lower() in ['quit', 'exit']:\n",
        "            print(\"Chatbot session ended. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        print(\"Chatbot: Thinking...\")\n",
        "\n",
        "        # Generate the RAG response\n",
        "        response = generate_rag_response(user_input)\n",
        "\n",
        "        print(f\"Chatbot: {response}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhNKx6KmnX73",
        "outputId": "6aafee21-17ec-4dda-d760-078b95e8aec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Eureka Forbes RAG Chatbot Initialized ---\n",
            "You can now ask questions about the Eureka Forbes documentation.\n",
            "Type 'quit' or 'exit' to end the session.\n",
            "--------------------------------------------------\n",
            "You: What is warranty and amc coverage\n",
            "Chatbot: Thinking...\n",
            "Chatbot: The warranty covers manufacturing defects, electrical failures, and service technician visits, but it does not cover consumables such as filters. Water purifiers typically have a 1-year warranty with extended warranty options, while vacuum cleaners have a 1–2 year warranty depending on the model.\n",
            "\n",
            "The Annual Maintenance Contract (AMC) includes periodic services, free filter replacement, free technician visits, and priority support, making it ideal for maintaining RO purifiers.\n",
            "\n",
            "You: What is the warranty on the RO purifier?\n",
            "Chatbot: Thinking...\n",
            "Chatbot: Eureka Forbes water purifiers come with a 1-year warranty, and an extended warranty is also available.\n",
            "\n",
            "You: Does it cover filters?\n",
            "Chatbot: Thinking...\n",
            "Chatbot: Filters are not covered under warranty as they are considered consumables. However, an Annual Maintenance Contract (AMC) includes free filter replacement.\n",
            "\n",
            "You: Is it free of cost?\n",
            "Chatbot: Thinking...\n",
            "Chatbot: Installation is free for most products. Please note that some models may require paid consumables such as pre-filter housing, extra pipes, or a booster pump.\n",
            "\n",
            "You: What is bumblebee filter\n",
            "Chatbot: Thinking...\n",
            "Chatbot: I apologize, but I don't have information about a \"bumblebee filter\" in my knowledge base. For further assistance, please call our toll-free number 1860 266 1177, which is also available on WhatsApp.\n",
            "\n",
            "You: Qui\n",
            "Chatbot: Thinking...\n",
            "Chatbot: I'm sorry, I don't have information about that. Please call our toll-free number at 1860 266 1177, which is also available on WhatsApp, for further assistance.\n",
            "\n",
            "You: Quit\n",
            "Chatbot session ended. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Getting conversational memory up**"
      ],
      "metadata": {
        "id": "Ekp3-hzdzH6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a new chat session when the program starts\n",
        "chat_session = client.chats.create(model=GENERATION_MODEL)\n",
        "\n",
        "print(\"✅ Chat session initialized for conversational memory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNxWPaH_rbpB",
        "outputId": "73a1b434-c34e-4b20-926f-9ea9ae7f8908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chat session initialized for conversational memory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Revised Generation Function for Chat History ---\n",
        "\n",
        "def generate_rag_response_with_history(query: str, chat_session_obj) -> str:\n",
        "    \"\"\"\n",
        "    Performs RAG and sends the context and query to the Gemini Chat session.\n",
        "    \"\"\"\n",
        "    # 1. Retrieval: Get the context based ONLY on the current user query\n",
        "    retrieved_context = get_rag_context(query)\n",
        "\n",
        "    # 2. Construct the full instruction prompt (System Prompt + Context)\n",
        "    # The actual chat history is managed automatically by chat_session_obj\n",
        "    system_instruction = (\n",
        "        \"You are an expert conversational AI bot for Eureka Forbes. \"\n",
        "        \"Your response MUST be based ONLY on the context provided in this prompt. \"\n",
        "        \"Do not use external knowledge. The context is enclosed in the CONTEXT START/END tags. \"\n",
        "        \"Keep the tone professional and helpful. Do not mention 'as per the data provided' or similar phrasing. \"\n",
        "        \"If the answer is not in the context, state that politely that you dont know about it and please call a toll free number 1860 266 1177 which is also available on WhatsApp.\"\n",
        "    )\n",
        "\n",
        "    # Combine the system instruction and context to guard the chat model\n",
        "    guarded_context_prompt = (\n",
        "        f\"{system_instruction}\\n\\n\"\n",
        "        f\"--- CONTEXT START ---\\n\"\n",
        "        f\"{retrieved_context}\\n\"\n",
        "        f\"--- CONTEXT END ---\\n\\n\"\n",
        "        f\"CUSTOMER QUERY: {query}\"\n",
        "    )\n",
        "\n",
        "    # 3. Generation: Send the message using the chat object (history is preserved)\n",
        "    try:\n",
        "        response = chat_session_obj.send_message(\n",
        "            guarded_context_prompt\n",
        "        )\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Sorry, an error occurred during generation: {e}\"\n",
        "\n",
        "print(\"✅ Generation function updated to utilize chat history.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_qT_Bn9xiLN",
        "outputId": "0b1b6427-5a26-424e-bca8-f0cb1c126b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Generation function updated to utilize chat history.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Revised Conversational Loop (Step 6) ---\n",
        "\n",
        "print(\"--- Eureka Forbes RAG Chatbot (with Memory) Initialized ---\")\n",
        "print(\"You can now ask follow-up questions (e.g., 'Does it cover filters?').\")\n",
        "print(\"Type 'quit' or 'exit' to end the session.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Start the interactive loop\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"You: \").strip()\n",
        "\n",
        "        if user_input.lower() in ['quit', 'exit']:\n",
        "            print(\"Chatbot session ended. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        print(\"Chatbot: Thinking...\")\n",
        "\n",
        "        # !!! Call the new function and pass the chat session !!!\n",
        "        response = generate_rag_response_with_history(user_input, chat_session)\n",
        "\n",
        "        print(f\"Chatbot: {response}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ2MMTe5x7OF",
        "outputId": "b6546fd0-0722-4fc4-83f5-5f31f3cab71c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Eureka Forbes RAG Chatbot (with Memory) Initialized ---\n",
            "You can now ask follow-up questions (e.g., 'Does it cover filters?').\n",
            "Type 'quit' or 'exit' to end the session.\n",
            "--------------------------------------------------\n",
            "You: What is the warranty on Eureka Forbes products?\n",
            "Chatbot: Thinking...\n",
            "Chatbot: The warranty for Eureka Forbes products is as follows:\n",
            "\n",
            "*   **Water purifiers:** 1 year warranty (+ extended warranty available)\n",
            "*   **Vacuum cleaners:** 1–2 years depending on the model\n",
            "\n",
            "You: Does it cover new filters?\n",
            "Chatbot: Thinking...\n",
            "Chatbot: The warranty covers manufacturing defects, electrical failures, and service technician visits. It does not cover consumables such as filters.\n",
            "\n",
            "You: Is it free of cost\n",
            "Chatbot: Thinking...\n",
            "Chatbot: For most products, installation is free.\n",
            "\n",
            "You: Does it come with the product or I have to buy it with the product?\n",
            "Chatbot: Thinking...\n",
            "Chatbot: For most products, installation is free and is usually completed within 24–48 hours after delivery.\n",
            "\n",
            "You: qUIT\n",
            "Chatbot session ended. Goodbye!\n"
          ]
        }
      ]
    }
  ]
}